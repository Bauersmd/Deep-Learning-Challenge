


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

# Import pandas and read the charity_data.csv from the provided cloud URL.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
#  YOUR CODE GOES HERE
application_df = application_df.drop(columns = ['EIN', 'NAME', 'STATUS', 'SPECIAL_CONSIDERATIONS', 'ORGANIZATION'])
application_df


# Determine the number of unique values in each column.
#  YOUR CODE GOES HERE
application_df.nunique()


# Look at APPLICATION_TYPE value counts to identify and replace with "Other"
#  YOUR CODE GOES HERE
application_type_counts = application_df['APPLICATION_TYPE'].value_counts()
application_type_counts


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
#  YOUR CODE GOES HERE
application_types_to_replace = list(application_type_counts[application_type_counts < 528].index)

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts to identify and replace with "Other"
#  YOUR CODE GOES HERE
classification_count_types = application_df['CLASSIFICATION'].value_counts()
classification_count_types


# You may find it helpful to look at CLASSIFICATION value counts >1
#  YOUR CODE GOES HERE
cleaned_classifications = classification_count_types.loc[classification_count_types > 1]
cleaned_classifications


# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
#  YOUR CODE GOES HERE
classifications_to_replace = list(classification_count_types[classification_count_types < 287].index)

# Replace in dataframe
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
application_df['CLASSIFICATION'].value_counts()


categorical_data = application_df.dtypes[application_df.dtypes == 'object'].index.tolist()
categorical_data


# Convert categorical data to numeric with `pd.get_dummies`
#  YOUR CODE GOES HERE
dummies_cleaned_df = pd.get_dummies(application_df)
dummies_cleaned_df


# Split our preprocessed data into our features and target arrays
#  YOUR CODE GOES HERE
X = dummies_cleaned_df.drop(['IS_SUCCESSFUL'], axis = 'columns').values
y = dummies_cleaned_df['IS_SUCCESSFUL'].values

# Split the preprocessed data into a training and testing dataset
#  YOUR CODE GOES HERE
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 5)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# First Attempt - Accuracy 72%
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE
input_values = len(X_train[0])
nodes_hidden_layer1 = 75
nodes_hidden_layer2 = 25

nn = tf.keras.models.Sequential()

# First hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer1, input_dim = input_values, activation = 'relu'))

# Second hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer2, activation = 'relu'))

# Output layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

# Check the structure of the model
nn.summary()


# Compile the model
#  YOUR CODE GOES HERE
nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# Train the model
#  YOUR CODE GOES HERE
training_model = nn.fit(X_train_scaled, y_train, epochs = 100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Second Attempt - Accuracy 72%
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE
input_values = len(X_train[0])
nodes_hidden_layer1 = 90
nodes_hidden_layer2 = 45
#nodes_hidden_layer3 = 15

nn = tf.keras.models.Sequential()

# First hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer1, input_dim = input_values, activation = 'relu'))

# Second hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer2, activation = 'relu'))

# Third hidden layer
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer2, activation = 'relu'))

# Output layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

# Check the structure of the model
nn.summary()


# Compile the model
#  YOUR CODE GOES HERE
nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# Train the model
#  YOUR CODE GOES HERE
training_model = nn.fit(X_train_scaled, y_train, epochs = 100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Third Attempt Accuracy 72%
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE
input_values = len(X_train[0])
nodes_hidden_layer1 = 25
nodes_hidden_layer2 = 10
#nodes_hidden_layer3 = 8

nn = tf.keras.models.Sequential()

# First hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer1, input_dim = input_values, activation = 'relu'))

# Second hidden layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer2, activation = 'relu'))

# Third hidden layer
nn.add(tf.keras.layers.Dense(units = nodes_hidden_layer2, activation = 'sigmoid'))

# Output layer
#  YOUR CODE GOES HERE
nn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

# Check the structure of the model
nn.summary()


# Compile the model
#  YOUR CODE GOES HERE
nn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# Train the model
#  YOUR CODE GOES HERE
training_model = nn.fit(X_train_scaled, y_train, epochs = 100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
#  YOUR CODE GOES HERE
nn.save('AlphabetSoupCharity_2_Optimization.h5')



